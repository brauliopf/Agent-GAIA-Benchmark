{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio to Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from groq import Groq\n",
    "\n",
    "def audio_2_text(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    transcribe an audio file to text\n",
    "    Args:\n",
    "      file_path (str): the absolute file_path of the targeted audio.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the Groq client\n",
    "    client = Groq()\n",
    "\n",
    "    # Open the audio file\n",
    "    with open(file_path, \"rb\") as file:\n",
    "        # Create a transcription of the audio file\n",
    "        transcription = client.audio.transcriptions.create(\n",
    "          file=file, # Required audio file\n",
    "          model=\"whisper-large-v3-turbo\", # Required model to use for transcription\n",
    "          prompt=\"Specify context or spelling\",  # Optional\n",
    "          response_format=\"verbose_json\",  # Optional\n",
    "          timestamp_granularities = [\"word\", \"segment\"], # Optional (must set response_format to \"json\" to use and can specify \"word\", \"segment\" (default), or both)\n",
    "          # language=\"en\",  # Optional\n",
    "          temperature=0.0  # Optional\n",
    "        )\n",
    "        # To print only the transcription text, you'd use print(transcription.text\n",
    "        # Print the entire transcription object to access timestamps\n",
    "    \n",
    "    return json.dumps(transcription.text, indent=2, default=str)\n",
    "\n",
    "audio_2_text('/Users/brauliopf/Documents/Dev/hf-agents/final/attachments/1f975693-876d-457b-a649-393859e79bf3.mp3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def code_executor(code: str, timeout: int = 10) -> dict:\n",
    "    \"\"\"\n",
    "    Executes Python code in a subprocess and returns the result.\n",
    "    \"\"\"\n",
    "    result = {\"stdout\": \"\", \"stderr\": \"\", \"exit_code\": 0}\n",
    "    try:\n",
    "        process = subprocess.run(\n",
    "            [sys.executable, \"-c\", code],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=timeout,\n",
    "            shell=False,\n",
    "        )\n",
    "        result.update({\n",
    "            \"stdout\": process.stdout,\n",
    "            \"stderr\": process.stderr,\n",
    "            \"exit_code\": process.returncode,\n",
    "        })\n",
    "    except subprocess.TimeoutExpired:\n",
    "        result[\"stderr\"] = \"Execution timed out.\"\n",
    "    except Exception as e:\n",
    "        result[\"stderr\"] = str(e)\n",
    "    return result\n",
    "\n",
    "def execute_code_from_file(file_path: str, timeout: int = 10) -> dict:\n",
    "    \"\"\"\n",
    "    Reads Python code from a file and executes it in a subprocess.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the Python file to execute\n",
    "        timeout (int): Maximum execution time in seconds\n",
    "        \n",
    "    Returns:\n",
    "        dict: Execution results including stdout, stderr, and exit code\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            code = f.read()\n",
    "        return code_executor(code, timeout)\n",
    "    except FileNotFoundError:\n",
    "        return {\"stdout\": \"\", \"stderr\": f\"File not found: {file_path}\", \"exit_code\": 1}\n",
    "    except Exception as e:\n",
    "        return {\"stdout\": \"\", \"stderr\": f\"Error reading file: {str(e)}\", \"exit_code\": 1}\n",
    "\n",
    "execute_code_from_file('/Users/brauliopf/Documents/Dev/hf-agents/final/attachments/f918266a-b3e0-4914-865d-4faa564f1aef.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import base64\n",
    "import os\n",
    "\n",
    "# Function to encode the image\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "# Getting the base64 string\n",
    "base64_image = encode_image(\"/Users/brauliopf/Documents/Dev/hf-agents/final/tmp/cca530fc-4052-43b2-b130-b30968d8aa44.png.png\")\n",
    "\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"What's in this image? Describe the image in detail. If this is a board game, read the current status of the board, but do not make an analysis at all\"},\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/jpeg;base64,{base64_image}\",\n",
    "                    },\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    "    model=\"gpt-4.1-2025-04-14\"\n",
    ")\n",
    "\n",
    "chat_completion.choices[0].message.content\n",
    "\n",
    "print(chat_completion.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from urllib.parse import urlparse\n",
    "import os\n",
    "import uuid\n",
    "import requests\n",
    "\n",
    "def download_file(url: str, filename: str) -> str:\n",
    "    \"\"\"\n",
    "    Downloads a file from the web and stores it in a file. Returns the absolute path for the file\n",
    "    Args:\n",
    "      url (str): the URL to download the target file.\n",
    "      filename (str): the name of the file, with file extension.\n",
    "    Returns:\n",
    "      file_path (str): the absolute path to the file.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get file extension\n",
    "    extension = filename.split('.')[1]\n",
    "\n",
    "    # Create temporary file\n",
    "    temp_dir = './tmp'\n",
    "    filepath = os.path.join(temp_dir, f'{filename}.{extension}')\n",
    "\n",
    "    # Download file\n",
    "    response = requests.get(url, stream=True)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    # Save file\n",
    "    with open(filepath, \"wb\") as f:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            f.write(chunk)\n",
    "\n",
    "furl = \"https://agents-course-unit4-scoring.hf.space/files/cca530fc-4052-43b2-b130-b30968d8aa44\"\n",
    "download_file(furl, \"cca530fc-4052-43b2-b130-b30968d8aa44.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tavily"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Tavily: returns an answer by an llm and a list of results, ranked by relevancy\n",
    "* DuckDuckGo: returns list (DuckDuckGoSearchResults) or text (DuckDuckGoSearchRun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To install: pip install tavily-python\n",
    "from tavily import TavilyClient\n",
    "tavily_client = TavilyClient(os.getenv('TAVILY_API_KEY'))\n",
    "response = tavily_client.search( # https://docs.tavily.com/documentation/api-reference/endpoint/search\n",
    "    query=\"Mercedes Sosa\",\n",
    "    max_results=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [result.get('url') for result in response.get('results')]\n",
    "\n",
    "# create embedding model\n",
    "from langchain_google_vertexai import VertexAIEmbeddings\n",
    "import vertexai\n",
    "vertexai.init(project=\"hf-agent-final\") # initialize VertexAI with your project ID\n",
    "embeddings = VertexAIEmbeddings(model=\"text-embedding-004\",) # create vectors of 768 dimensions\n",
    "\n",
    "# create vector_store\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "vector_store = InMemoryVectorStore(embeddings)\n",
    "\n",
    "# load content from target websites (WebBaseLoader + BS4)\n",
    "import bs4\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = WebBaseLoader( # use urllib to load HTML from web URLs and BeautifulSoup to parse it to text.\n",
    "    web_paths=urls,\n",
    "    bs_get_text_kwargs={\"separator\": \" | \", \"strip\":True}\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "# embed contents (docs)\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=100, chunk_overlap=50\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Instantiate embedding model (VertexAI + project ID)\n",
    "vertexai.init(project=\"hf-agent-final\")\n",
    "embeddings = VertexAIEmbeddings(model=\"text-embedding-004\",) # creates vectors of 768 dimensions\n",
    "\n",
    "# Add to vectorDB (embbed on input and store)\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=doc_splits,\n",
    "    collection_name=\"rag-chroma\",\n",
    "    embedding=embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"retrieve_web_content\",\n",
    "    \"Search and return information about the topic of interest on pages suggested as relevant by the web search tool (Tavily)\",\n",
    ")\n",
    "\n",
    "retriever_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DuckDuckGo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import DuckDuckGoSearchRun, DuckDuckGoSearchResults\n",
    "\n",
    "search = DuckDuckGoSearchRun(output_format=\"list\")\n",
    "\n",
    "search.invoke(\"Obama's first name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wikipedia App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wikipedia app?\n",
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "\n",
    "wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\n",
    "\n",
    "wikipedia.run(\"Mercedes Sosa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YouTube to Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "\n",
    "def youtube_to_text(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Transcribe video from a youtube URL. Processes a single video at a time.\n",
    "    Args:\n",
    "        url (str): the URL of the target video. Must be a youtube URL. Example: https://www.youtube.com/watch?v=ABC155LMLVL\n",
    "    \"\"\"\n",
    "\n",
    "    # Get API key from environment (safer than hardcoding)\n",
    "    genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\"))  # Set in environment\n",
    "    \n",
    "    # 1. Extract video ID from URL\n",
    "    video_id = url.split(\"v=\")[1].split(\"&\")[0]\n",
    "    \n",
    "    try:\n",
    "        # 2. Get existing captions (if available)\n",
    "        transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
    "    except:\n",
    "        # 3. If no captions, use Whisper/other ASR here (requires audio download)\n",
    "        raise Exception(\"No captions found - need audio transcription implementation\")\n",
    "    \n",
    "    # 4. Format transcript text\n",
    "    transcript_text = \" \".join([entry[\"text\"] for entry in transcript])\n",
    "    \n",
    "    # 5. Use Gemini to process/formalize transcript\n",
    "    model = genai.GenerativeModel('gemini-2.0-flash')  # Use appropriate model\n",
    "    prompt = f\"\"\"Format this video transcript with speaker identification:\n",
    "    {transcript_text}\n",
    "    \"\"\"\n",
    "    \n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text\n",
    "\n",
    "youtube_to_text(\"https://www.youtube.com/watch?v=CSvFpBOe8eY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflows and Agents\n",
    "\n",
    "*** Ref: https://langchain-ai.github.io/langgraph/tutorials/workflows/\n",
    "\n",
    "Workflows are systems where LLMs and tools are orchestrated through predefined code paths.\n",
    "Agents, on the other hand, are systems where LLMs dynamically direct their own processes and tool usage, maintaining control over how they accomplish tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "# from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "\n",
    "# _set_env(\"ANTHROPIC_API_KEY\")\n",
    "\n",
    "# llm = ChatAnthropic(model=\"claude-3-5-sonnet-latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema for structured output\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class SearchQuery(BaseModel):\n",
    "    search_query: str = Field(None, description=\"Query that is optimized web search.\")\n",
    "    justification: str = Field(\n",
    "        None, description=\"Why this query is relevant to the user's request.\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Augment the LLM with schema for structured output\n",
    "structured_llm = llm.with_structured_output(SearchQuery)\n",
    "\n",
    "# Invoke the augmented LLM\n",
    "output = structured_llm.invoke(\"How does Calcium CT score relate to high cholesterol?\")\n",
    "\n",
    "# Define a tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    return a * b\n",
    "\n",
    "# Augment the LLM with tools\n",
    "llm_with_tools = llm.bind_tools([multiply])\n",
    "\n",
    "# Invoke the LLM with input that triggers the tool call\n",
    "msg = llm_with_tools.invoke(\"What is 2 times 3?\")\n",
    "\n",
    "# Get the tool call\n",
    "msg.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from IPython.display import Image, display\n",
    "\n",
    "\n",
    "# Graph state\n",
    "class State(TypedDict):\n",
    "    topic: str\n",
    "    joke: str\n",
    "    improved_joke: str\n",
    "    final_joke: str\n",
    "\n",
    "\n",
    "# Nodes\n",
    "def generate_joke(state: State):\n",
    "    \"\"\"First LLM call to generate initial joke\"\"\"\n",
    "\n",
    "    msg = llm.invoke(f\"Write a short joke about {state['topic']}\")\n",
    "    return {\"joke\": msg.content}\n",
    "\n",
    "\n",
    "def check_punchline(state: State):\n",
    "    \"\"\"Gate function to check if the joke has a punchline\"\"\"\n",
    "\n",
    "    # Simple check - does the joke contain \"?\" or \"!\"\n",
    "    if \"?\" in state[\"joke\"] or \"!\" in state[\"joke\"]:\n",
    "        return \"Fail\"\n",
    "    return \"Pass\"\n",
    "\n",
    "\n",
    "def improve_joke(state: State):\n",
    "    \"\"\"Second LLM call to improve the joke\"\"\"\n",
    "\n",
    "    msg = llm.invoke(f\"Make this joke funnier by adding wordplay: {state['joke']}\")\n",
    "    return {\"improved_joke\": msg.content}\n",
    "\n",
    "\n",
    "def polish_joke(state: State):\n",
    "    \"\"\"Third LLM call for final polish\"\"\"\n",
    "\n",
    "    msg = llm.invoke(f\"Add a surprising twist to this joke: {state['improved_joke']}\")\n",
    "    return {\"final_joke\": msg.content}\n",
    "\n",
    "\n",
    "# Build workflow\n",
    "workflow = StateGraph(State)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"generate_joke\", generate_joke)\n",
    "workflow.add_node(\"improve_joke\", improve_joke)\n",
    "workflow.add_node(\"polish_joke\", polish_joke)\n",
    "\n",
    "# Add edges to connect nodes\n",
    "workflow.add_edge(START, \"generate_joke\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate_joke\", check_punchline, {\"Fail\": \"improve_joke\", \"Pass\": END}\n",
    ")\n",
    "workflow.add_edge(\"improve_joke\", \"polish_joke\")\n",
    "workflow.add_edge(\"polish_joke\", END)\n",
    "\n",
    "# Compile\n",
    "chain = workflow.compile()\n",
    "\n",
    "# Show workflow\n",
    "display(Image(chain.get_graph().draw_mermaid_png()))\n",
    "\n",
    "# Invoke\n",
    "state = chain.invoke({\"topic\": \"cats\"})\n",
    "print(\"Initial joke:\")\n",
    "print(state[\"joke\"])\n",
    "print(\"\\n--- --- ---\\n\")\n",
    "if \"improved_joke\" in state:\n",
    "    print(\"Improved joke:\")\n",
    "    print(state[\"improved_joke\"])\n",
    "    print(\"\\n--- --- ---\\n\")\n",
    "\n",
    "    print(\"Final joke:\")\n",
    "    print(state[\"final_joke\"])\n",
    "else:\n",
    "    print(\"Joke failed quality gate - no punchline detected!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallelization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph state\n",
    "class State(TypedDict):\n",
    "    topic: str\n",
    "    joke: str\n",
    "    story: str\n",
    "    poem: str\n",
    "    combined_output: str\n",
    "\n",
    "\n",
    "# Nodes\n",
    "def call_llm_1(state: State):\n",
    "    \"\"\"First LLM call to generate initial joke\"\"\"\n",
    "\n",
    "    msg = llm.invoke(f\"Write a joke about {state['topic']}\")\n",
    "    return {\"joke\": msg.content}\n",
    "\n",
    "\n",
    "def call_llm_2(state: State):\n",
    "    \"\"\"Second LLM call to generate story\"\"\"\n",
    "\n",
    "    msg = llm.invoke(f\"Write a story about {state['topic']}\")\n",
    "    return {\"story\": msg.content}\n",
    "\n",
    "\n",
    "def call_llm_3(state: State):\n",
    "    \"\"\"Third LLM call to generate poem\"\"\"\n",
    "\n",
    "    msg = llm.invoke(f\"Write a poem about {state['topic']}\")\n",
    "    return {\"poem\": msg.content}\n",
    "\n",
    "\n",
    "def aggregator(state: State):\n",
    "    \"\"\"Combine the joke and story into a single output\"\"\"\n",
    "\n",
    "    combined = f\"Here's a story, joke, and poem about {state['topic']}!\\n\\n\"\n",
    "    combined += f\"STORY:\\n{state['story']}\\n\\n\"\n",
    "    combined += f\"JOKE:\\n{state['joke']}\\n\\n\"\n",
    "    combined += f\"POEM:\\n{state['poem']}\"\n",
    "    return {\"combined_output\": combined}\n",
    "\n",
    "\n",
    "# Build workflow\n",
    "parallel_builder = StateGraph(State)\n",
    "\n",
    "# Add nodes\n",
    "parallel_builder.add_node(\"call_llm_1\", call_llm_1)\n",
    "parallel_builder.add_node(\"call_llm_2\", call_llm_2)\n",
    "parallel_builder.add_node(\"call_llm_3\", call_llm_3)\n",
    "parallel_builder.add_node(\"aggregator\", aggregator)\n",
    "\n",
    "# Add edges to connect nodes\n",
    "parallel_builder.add_edge(START, \"call_llm_1\")\n",
    "parallel_builder.add_edge(START, \"call_llm_2\")\n",
    "parallel_builder.add_edge(START, \"call_llm_3\")\n",
    "parallel_builder.add_edge(\"call_llm_1\", \"aggregator\")\n",
    "parallel_builder.add_edge(\"call_llm_2\", \"aggregator\")\n",
    "parallel_builder.add_edge(\"call_llm_3\", \"aggregator\")\n",
    "parallel_builder.add_edge(\"aggregator\", END)\n",
    "parallel_workflow = parallel_builder.compile()\n",
    "\n",
    "# Show workflow\n",
    "display(Image(parallel_workflow.get_graph().draw_mermaid_png()))\n",
    "\n",
    "# Invoke\n",
    "state = parallel_workflow.invoke({\"topic\": \"cats\"})\n",
    "print(state[\"combined_output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import Literal\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "\n",
    "# Schema for structured output to use as routing logic\n",
    "class Route(BaseModel):\n",
    "    step: Literal[\"poem\", \"story\", \"joke\"] = Field(\n",
    "        None, description=\"The next step in the routing process\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Augment the LLM with schema for structured output\n",
    "router = llm.with_structured_output(Route)\n",
    "\n",
    "\n",
    "# State\n",
    "class State(TypedDict):\n",
    "    input: str\n",
    "    decision: str\n",
    "    output: str\n",
    "\n",
    "\n",
    "# Nodes\n",
    "def llm_call_1(state: State):\n",
    "    \"\"\"Write a story\"\"\"\n",
    "\n",
    "    result = llm.invoke(state[\"input\"])\n",
    "    return {\"output\": result.content}\n",
    "\n",
    "\n",
    "def llm_call_2(state: State):\n",
    "    \"\"\"Write a joke\"\"\"\n",
    "\n",
    "    result = llm.invoke(state[\"input\"])\n",
    "    return {\"output\": result.content}\n",
    "\n",
    "\n",
    "def llm_call_3(state: State):\n",
    "    \"\"\"Write a poem\"\"\"\n",
    "\n",
    "    result = llm.invoke(state[\"input\"])\n",
    "    return {\"output\": result.content}\n",
    "\n",
    "\n",
    "def llm_call_router(state: State):\n",
    "    \"\"\"Route the input to the appropriate node\"\"\"\n",
    "\n",
    "    # Run the augmented LLM with structured output to serve as routing logic\n",
    "    decision = router.invoke(\n",
    "        [\n",
    "            SystemMessage(\n",
    "                content=\"Route the input to story, joke, or poem based on the user's request.\"\n",
    "            ),\n",
    "            HumanMessage(content=state[\"input\"]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return {\"decision\": decision.step}\n",
    "\n",
    "\n",
    "# Conditional edge function to route to the appropriate node\n",
    "def route_decision(state: State):\n",
    "    # Return the node name you want to visit next\n",
    "    if state[\"decision\"] == \"story\":\n",
    "        return \"llm_call_1\"\n",
    "    elif state[\"decision\"] == \"joke\":\n",
    "        return \"llm_call_2\"\n",
    "    elif state[\"decision\"] == \"poem\":\n",
    "        return \"llm_call_3\"\n",
    "\n",
    "\n",
    "# Build workflow\n",
    "router_builder = StateGraph(State)\n",
    "\n",
    "# Add nodes\n",
    "router_builder.add_node(\"llm_call_1\", llm_call_1)\n",
    "router_builder.add_node(\"llm_call_2\", llm_call_2)\n",
    "router_builder.add_node(\"llm_call_3\", llm_call_3)\n",
    "router_builder.add_node(\"llm_call_router\", llm_call_router)\n",
    "\n",
    "# Add edges to connect nodes\n",
    "router_builder.add_edge(START, \"llm_call_router\")\n",
    "router_builder.add_conditional_edges(\n",
    "    \"llm_call_router\",\n",
    "    route_decision,\n",
    "    {  # Name returned by route_decision : Name of next node to visit\n",
    "        \"llm_call_1\": \"llm_call_1\",\n",
    "        \"llm_call_2\": \"llm_call_2\",\n",
    "        \"llm_call_3\": \"llm_call_3\",\n",
    "    },\n",
    ")\n",
    "router_builder.add_edge(\"llm_call_1\", END)\n",
    "router_builder.add_edge(\"llm_call_2\", END)\n",
    "router_builder.add_edge(\"llm_call_3\", END)\n",
    "\n",
    "# Compile workflow\n",
    "router_workflow = router_builder.compile()\n",
    "\n",
    "# Show the workflow\n",
    "display(Image(router_workflow.get_graph().draw_mermaid_png()))\n",
    "\n",
    "# Invoke\n",
    "state = router_workflow.invoke({\"input\": \"Write me a joke about cats\"})\n",
    "print(state[\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orchestrator-Worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, List\n",
    "import operator\n",
    "\n",
    "\n",
    "# Schema for structured output to use in planning\n",
    "class Section(BaseModel):\n",
    "    name: str = Field(\n",
    "        description=\"Name for this section of the report.\",\n",
    "    )\n",
    "    description: str = Field(\n",
    "        description=\"Brief overview of the main topics and concepts to be covered in this section.\",\n",
    "    )\n",
    "\n",
    "\n",
    "class Sections(BaseModel):\n",
    "    sections: List[Section] = Field(\n",
    "        description=\"Sections of the report.\",\n",
    "    )\n",
    "\n",
    "\n",
    "# Augment the LLM with schema for structured output\n",
    "planner = llm.with_structured_output(Sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.constants import Send\n",
    "\n",
    "\n",
    "# Graph state\n",
    "class State(TypedDict):\n",
    "    topic: str  # Report topic\n",
    "    sections: list[Section]  # List of report sections\n",
    "    completed_sections: Annotated[\n",
    "        list, operator.add\n",
    "    ]  # All workers write to this key in parallel\n",
    "    final_report: str  # Final report\n",
    "\n",
    "\n",
    "# Worker state\n",
    "class WorkerState(TypedDict):\n",
    "    section: Section\n",
    "    completed_sections: Annotated[list, operator.add]\n",
    "\n",
    "\n",
    "# Nodes\n",
    "def orchestrator(state: State):\n",
    "    \"\"\"Orchestrator that generates a plan for the report\"\"\"\n",
    "\n",
    "    # Generate queries\n",
    "    report_sections = planner.invoke(\n",
    "        [\n",
    "            SystemMessage(content=\"Generate a plan for the report.\"),\n",
    "            HumanMessage(content=f\"Here is the report topic: {state['topic']}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return {\"sections\": report_sections.sections}\n",
    "\n",
    "\n",
    "def llm_call(state: WorkerState):\n",
    "    \"\"\"Worker writes a section of the report\"\"\"\n",
    "\n",
    "    # Generate section\n",
    "    section = llm.invoke(\n",
    "        [\n",
    "            SystemMessage(\n",
    "                content=\"Write a report section following the provided name and description. Include no preamble for each section. Use markdown formatting.\"\n",
    "            ),\n",
    "            HumanMessage(\n",
    "                content=f\"Here is the section name: {state['section'].name} and description: {state['section'].description}\"\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Write the updated section to completed sections\n",
    "    return {\"completed_sections\": [section.content]}\n",
    "\n",
    "\n",
    "def synthesizer(state: State):\n",
    "    \"\"\"Synthesize full report from sections\"\"\"\n",
    "\n",
    "    # List of completed sections\n",
    "    completed_sections = state[\"completed_sections\"]\n",
    "\n",
    "    # Format completed section to str to use as context for final sections\n",
    "    completed_report_sections = \"\\n\\n---\\n\\n\".join(completed_sections)\n",
    "\n",
    "    return {\"final_report\": completed_report_sections}\n",
    "\n",
    "\n",
    "# Conditional edge function to create llm_call workers that each write a section of the report\n",
    "def assign_workers(state: State):\n",
    "    \"\"\"Assign a worker to each section in the plan\"\"\"\n",
    "\n",
    "    # Kick off section writing in parallel via Send() API\n",
    "    return [Send(\"llm_call\", {\"section\": s}) for s in state[\"sections\"]]\n",
    "\n",
    "\n",
    "# Build workflow\n",
    "orchestrator_worker_builder = StateGraph(State)\n",
    "\n",
    "# Add the nodes\n",
    "orchestrator_worker_builder.add_node(\"orchestrator\", orchestrator)\n",
    "orchestrator_worker_builder.add_node(\"llm_call\", llm_call)\n",
    "orchestrator_worker_builder.add_node(\"synthesizer\", synthesizer)\n",
    "\n",
    "# Add edges to connect nodes\n",
    "orchestrator_worker_builder.add_edge(START, \"orchestrator\")\n",
    "orchestrator_worker_builder.add_conditional_edges(\n",
    "    \"orchestrator\", assign_workers, [\"llm_call\"]\n",
    ")\n",
    "orchestrator_worker_builder.add_edge(\"llm_call\", \"synthesizer\")\n",
    "orchestrator_worker_builder.add_edge(\"synthesizer\", END)\n",
    "\n",
    "# Compile the workflow\n",
    "orchestrator_worker = orchestrator_worker_builder.compile()\n",
    "\n",
    "# Show the workflow\n",
    "display(Image(orchestrator_worker.get_graph().draw_mermaid_png()))\n",
    "\n",
    "# Invoke\n",
    "state = orchestrator_worker.invoke({\"topic\": \"Create a report on LLM scaling laws\"})\n",
    "\n",
    "from IPython.display import Markdown\n",
    "Markdown(state[\"final_report\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluator-optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph state\n",
    "class State(TypedDict):\n",
    "    joke: str\n",
    "    topic: str\n",
    "    feedback: str\n",
    "    funny_or_not: str\n",
    "\n",
    "\n",
    "# Schema for structured output to use in evaluation\n",
    "class Feedback(BaseModel):\n",
    "    grade: Literal[\"funny\", \"not funny\"] = Field(\n",
    "        description=\"Decide if the joke is funny or not.\",\n",
    "    )\n",
    "    feedback: str = Field(\n",
    "        description=\"If the joke is not funny, provide feedback on how to improve it.\",\n",
    "    )\n",
    "\n",
    "\n",
    "# Augment the LLM with schema for structured output\n",
    "evaluator = llm.with_structured_output(Feedback)\n",
    "\n",
    "\n",
    "# Nodes\n",
    "def llm_call_generator(state: State):\n",
    "    \"\"\"LLM generates a joke\"\"\"\n",
    "\n",
    "    if state.get(\"feedback\"):\n",
    "        msg = llm.invoke(\n",
    "            f\"Write a joke about {state['topic']} but take into account the feedback: {state['feedback']}\"\n",
    "        )\n",
    "    else:\n",
    "        msg = llm.invoke(f\"Write a joke about {state['topic']}\")\n",
    "    return {\"joke\": msg.content}\n",
    "\n",
    "\n",
    "def llm_call_evaluator(state: State):\n",
    "    \"\"\"LLM evaluates the joke\"\"\"\n",
    "\n",
    "    grade = evaluator.invoke(f\"Grade the joke {state['joke']}\")\n",
    "    return {\"funny_or_not\": grade.grade, \"feedback\": grade.feedback}\n",
    "\n",
    "\n",
    "# Conditional edge function to route back to joke generator or end based upon feedback from the evaluator\n",
    "def route_joke(state: State):\n",
    "    \"\"\"Route back to joke generator or end based upon feedback from the evaluator\"\"\"\n",
    "\n",
    "    if state[\"funny_or_not\"] == \"funny\":\n",
    "        return \"Accepted\"\n",
    "    elif state[\"funny_or_not\"] == \"not funny\":\n",
    "        return \"Rejected + Feedback\"\n",
    "\n",
    "\n",
    "# Build workflow\n",
    "optimizer_builder = StateGraph(State)\n",
    "\n",
    "# Add the nodes\n",
    "optimizer_builder.add_node(\"llm_call_generator\", llm_call_generator)\n",
    "optimizer_builder.add_node(\"llm_call_evaluator\", llm_call_evaluator)\n",
    "\n",
    "# Add edges to connect nodes\n",
    "optimizer_builder.add_edge(START, \"llm_call_generator\")\n",
    "optimizer_builder.add_edge(\"llm_call_generator\", \"llm_call_evaluator\")\n",
    "optimizer_builder.add_conditional_edges(\n",
    "    \"llm_call_evaluator\",\n",
    "    route_joke,\n",
    "    {  # Name returned by route_joke : Name of next node to visit\n",
    "        \"Accepted\": END,\n",
    "        \"Rejected + Feedback\": \"llm_call_generator\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# Compile the workflow\n",
    "optimizer_workflow = optimizer_builder.compile()\n",
    "\n",
    "# Show the workflow\n",
    "display(Image(optimizer_workflow.get_graph().draw_mermaid_png()))\n",
    "\n",
    "# Invoke\n",
    "state = optimizer_workflow.invoke({\"topic\": \"Cats\"})\n",
    "print(state[\"joke\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent\n",
    "\n",
    "*** Ref: https://langchain-ai.github.io/langgraph/tutorials/workflows/#orchestrator-worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "# Define tools\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply a and b.\n",
    "\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "@tool\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Adds a and b.\n",
    "\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "@tool\n",
    "def divide(a: int, b: int) -> float:\n",
    "    \"\"\"Divide a and b.\n",
    "\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a / b\n",
    "\n",
    "\n",
    "# Augment the LLM with tools\n",
    "tools = [add, multiply, divide]\n",
    "tools_by_name = {tool.name: tool for tool in tools}\n",
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import MessagesState\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, ToolMessage\n",
    "\n",
    "\n",
    "# Nodes\n",
    "def llm_call(state: MessagesState):\n",
    "    \"\"\"LLM decides whether to call a tool or not\"\"\"\n",
    "\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            llm_with_tools.invoke(\n",
    "                [\n",
    "                    SystemMessage(\n",
    "                        content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\n",
    "                    )\n",
    "                ]\n",
    "                + state[\"messages\"]\n",
    "            )\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "def tool_node(state: dict):\n",
    "    \"\"\"Performs the tool call\"\"\"\n",
    "\n",
    "    result = []\n",
    "    for tool_call in state[\"messages\"][-1].tool_calls:\n",
    "        tool = tools_by_name[tool_call[\"name\"]]\n",
    "        observation = tool.invoke(tool_call[\"args\"])\n",
    "        result.append(ToolMessage(content=observation, tool_call_id=tool_call[\"id\"]))\n",
    "    return {\"messages\": result}\n",
    "\n",
    "\n",
    "# Conditional edge function to route to the tool node or end based upon whether the LLM made a tool call\n",
    "def should_continue(state: MessagesState) -> Literal[\"environment\", END]:\n",
    "    \"\"\"Decide if we should continue the loop or stop and follow up with a tool call\"\"\"\n",
    "\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    # If the LLM makes a tool call, then perform an action\n",
    "    if last_message.tool_calls:\n",
    "        return \"Action\"\n",
    "    # Otherwise, we stop (reply to the user)\n",
    "    return END\n",
    "\n",
    "\n",
    "# Build workflow\n",
    "agent_builder = StateGraph(MessagesState)\n",
    "\n",
    "# Add nodes\n",
    "agent_builder.add_node(\"llm_call\", llm_call)\n",
    "agent_builder.add_node(\"environment\", tool_node)\n",
    "\n",
    "# Add edges to connect nodes\n",
    "agent_builder.add_edge(START, \"llm_call\")\n",
    "# Use a dictionary to map the results of a call to should_continue (Action x END)\n",
    "agent_builder.add_conditional_edges(\n",
    "    \"llm_call\",\n",
    "    should_continue,\n",
    "    {\n",
    "        # Name returned by should_continue : Name of next node to visit\n",
    "        \"Action\": \"environment\",\n",
    "        END: END,\n",
    "    },\n",
    ")\n",
    "agent_builder.add_edge(\"environment\", \"llm_call\")\n",
    "\n",
    "# Compile the agent\n",
    "agent = agent_builder.compile()\n",
    "\n",
    "# Show the agent\n",
    "display(Image(agent.get_graph(xray=True).draw_mermaid_png()))\n",
    "\n",
    "# Invoke\n",
    "messages = [HumanMessage(content=\"Add 3 and 4.\")]\n",
    "messages = agent.invoke({\"messages\": messages})\n",
    "for m in messages[\"messages\"]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "# Pass in:\n",
    "# (1) the augmented LLM with tools\n",
    "# (2) the tools list (which is used to create the tool node)\n",
    "pre_built_agent = create_react_agent(llm, tools=tools)\n",
    "\n",
    "# Show the agent\n",
    "display(Image(pre_built_agent.get_graph().draw_mermaid_png()))\n",
    "\n",
    "# Invoke\n",
    "messages = [HumanMessage(content=\"Add 3 and 4, then multiply by 5, and get the modulo 3.\")]\n",
    "messages = pre_built_agent.invoke({\"messages\": messages})\n",
    "for m in messages[\"messages\"]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supabase loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from supabase import create_client\n",
    "from langchain_community.vectorstores import SupabaseVectorStore\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "# Your Supabase project credentials\n",
    "supabase_url = os.getenv('SUPABASE_URL')\n",
    "supabase_key = os.getenv('SUPABASE_SERVICE_KEY')  # or service_role key for admin access\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\") #  dim=768\n",
    "\n",
    "# Initialize client\n",
    "supabase = create_client(supabase_url, supabase_key)\n",
    "\n",
    "vector_store = SupabaseVectorStore(\n",
    "    client=supabase,\n",
    "    embedding= embeddings,\n",
    "    table_name=\"documents\",\n",
    "    query_name=\"match_documents_langchain\",\n",
    ")\n",
    "create_retriever_tool = create_retriever_tool(\n",
    "    retriever=vector_store.as_retriever(),\n",
    "    name=\"Question Search\",\n",
    "    description=\"A tool to retrieve similar questions from a vector store.\",\n",
    ")\n",
    "\n",
    "\n",
    "# Get all data from a table\n",
    "response = supabase.table(\"items\").select(\"*\").execute()\n",
    "\n",
    "# Access the data\n",
    "data = response.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"The attached Excel file contains the sales of menu items for a local fast-food chain. What were the total sales that the chain made from food (not including drinks)? Express your answer in USD with two decimal places.\"\n",
    "\n",
    "from vertexai.language_mode\n",
    "import TextEmbeddingModel\n",
    "import vertexai\n",
    "\n",
    "vertexai.init(project=\"hf-agent-final\") # initialize VertexAI with your project ID\n",
    "model = TextEmbeddingModel.from_pretrained(\"textembedding-gecko@003\")\n",
    "response = model.get_embeddings(query)\n",
    "# embeddings = VertexAIEmbeddings(model=\"text-embedding-004\",) # create vectors of 768 dimensions\n",
    "\n",
    "# Generate embeddings (assuming 'text' column)\n",
    "def get_embedding(text):\n",
    "    response = openai.Embedding.create(\n",
    "        input=text,\n",
    "        model=\"text-embedding-3-small\"  # or another model\n",
    "    )\n",
    "    return response['data'][0]['embedding']\n",
    "\n",
    "def search_similar(query_text, top_k=5):\n",
    "    query_embedding = get_embedding(query_text)\n",
    "    query = f\"\"\"\n",
    "        select id, content, metadata\n",
    "        from items\n",
    "        order by embedding <-> '{query_embedding}'\n",
    "        limit {top_k};\n",
    "    \"\"\"\n",
    "    response = supabase.rpc(\"sql\", {\"query\": query}).execute()\n",
    "    return response.data\n",
    "\n",
    "results = search_similar(query)\n",
    "for r in results:\n",
    "    print(r['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brauliopf/Documents/Dev/hf-agents/final/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'89706.00'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from langchain_community.vectorstores import SupabaseVectorStore\n",
    "from supabase.client import Client, create_client\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "query = \"The attached Excel file contains the sales of menu items for a local fast-food chain. What were the total sales that the chain made from food (not including drinks)? Express your answer in USD with two decimal places.\"\n",
    "\n",
    "# build a retriever\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\") #  dim=768\n",
    "supabase: Client = create_client(\n",
    "    os.environ.get(\"SUPABASE_URL\"), \n",
    "    os.environ.get(\"SUPABASE_SERVICE_KEY\"))\n",
    "vector_store = SupabaseVectorStore(\n",
    "    client=supabase,\n",
    "    embedding= embeddings,\n",
    "    table_name=\"items\",\n",
    "    query_name=\"match_documents_langchain\",\n",
    ")\n",
    "create_retriever_tool = create_retriever_tool(\n",
    "    retriever=vector_store.as_retriever(),\n",
    "    name=\"Question Search\",\n",
    "    description=\"A tool to retrieve similar questions from a vector store.\",\n",
    ")\n",
    "\n",
    "def retriever(query):\n",
    "    similar_doc = vector_store.similarity_search(query, k=1)[0]\n",
    "\n",
    "    content = similar_doc.page_content\n",
    "    if \"Final answer :\" in content:\n",
    "        answer = content.split(\"Final answer :\")[-1].strip()\n",
    "    else:\n",
    "        answer = content.strip()\n",
    "\n",
    "    return answer\n",
    "\n",
    "retriever(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
